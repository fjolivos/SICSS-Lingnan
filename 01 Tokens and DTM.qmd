---
title: "Tokenization and DTM"
date: June 16, 2025
instructor: Francisco Olivos
---

# Introduction

Install and load the packages used in this session. Most of them are also required in the other sessions.

```{r setup, include=FALSE}
install.packages("remotes")
install.packages("tibble")
install.packages("dplyr")
install.packages("tokenizers")
install.packages("reshape2")
install.packages("quanteda")
install.packages("text2map")
install.packages("tidytext")
install.packages("ggplot2")
install.packages("patchwork")
install.packages("wordcloud")

library(remotes)
library(tibble)
library(dplyr)
library(tokenizers)
library(reshape2)
library(quanteda)
library(text2map)
library(tidytext)
library(ggplot2)
library(patchwork)
library(wordcloud)
```

# Tokenizing

Tokenization is the first step in computational text analysis. It involves breaking down a block of text into smaller units called **tokens**, usually words or phrases. These tokens serve as the basic building blocks for further analysis, such as counting word frequencies, building document-term matrices, or applying machine learning models.

Let's use Beyonce's songs:

```{r}

remotes::install_gitlab("culturalcartography/text2map.corpora")
data("corpus_beyonce", package = "text2map.corpora")
df_beyonce <- corpus_beyonce # Rename it to make tidier
dim(df_beyonce) # Count of row and columns

# Tokenize by splitting into words on the single space
head(strsplit(df_beyonce$song_text[4], " ", fixed = TRUE)[[1]], 50)  # [4] is "Crazy in Love"

# Tokenize by splitting into words by regex whitespace
head(strsplit(df_beyonce$song_text[4], "\\s+")[[1]], 50)  # \\s means any whitespace and + means one or more times

# Tokenize by splitting on non-word characters
head(strsplit(df_beyonce$song_text[4], "\\W")[[1]], 50)  # \\W means any non-word character

# Tokenize by splitting on non-alphanumerics
head(strsplit(df_beyonce$song_text[4], "[^a-zA-Z0-9]+")[[1]], 50)  # We can negate a match with [^]
```

Tokenization can be adapted to suit different research goals and linguistic contexts as above. However, we can also use the **tokenizers package**. Moreover, while word tokenization is common in English-language analysis, other languages or tasks may require alternative approaches. For instance, we may need to tokenize using **bigrams**, a combination of unigrams and bigrams, individual **characters** (useful for languages like Chinese), or **character clusters** (or shingles).

```{r}

# Word tokenization
head(tokenize_words(df_beyonce$song_text[4])[[1]], 50)

# Tokenize into bigrams
head(tokenize_ngrams(df_beyonce$song_text[4], n = 2L)[[1]], 50)

# Tokenize into unigrams and bigrams
head(tokenize_ngrams(df_beyonce$song_text[4], n = 2L, n_min = 1L)[[1]], 50)

# Tokenize by each character (very important in Chinese)
head(tokenize_characters(df_beyonce$song_text[4])[[1]], 50)

# Tokenize by cluster of 3 characters
head(tokenize_character_shingles(df_beyonce$song_text[4], n = 3, n_min = 3)[[1]], 50)
```

# Bonus: Try it out!

Tokenize the television script of "Star Trek: The Next Generation scripts." You can do it by unigram, biagrams, characters, or all of them.

If you do not know Start Trek, you must watch the trailer of this classic: <https://www.youtube.com/watch?v=HnDtvZXYHgE>

```{r}

data("corpus_tng_season5", package = "text2map.corpora")  

```

# Document Features

## Let's use The True Story of Ah Q, Lu Xun

The example below shows how to tokenize a single document and generate a simple **frequency count of words**. We use the `tokenize_words()` function to break the text into tokens, and then apply `table()` to count how often each token appears. This process creates a basic word frequency table, which is a common starting point in text analysis

Who is Lu Xun? <https://www.britannica.com/biography/Lu-Xun>

```{r}

doc <- "For several years now I have been meaning to write the true story of Ah Q. But while wanting to write I was in some trepidation, too, which goes to show that I am not one of those who achieve glory by writing; for an immortal pen has always been required to record the deeds of an immortal man, the man becoming known to posterity through the writing and the writing known to posterity through the manâ€”until finally it is not clear who is making whom known. But in the end, as though possessed by some fiend, I always came back to the idea of writing the story of Ah Q"

# Tokenize with the tokenize function (easier)
docs_tokens <- tokenize_words(doc)
class(docs_tokens)                       # The output is a token list

# Create a token-count list
docs_freqs <- table(docs_tokens)

# Look at the first few
docs_freqs[1:10]

# Generate a word cloud
wordcloud(names(docs_freqs), freq = as.numeric(docs_freqs), min.freq = 1)
```

If you got here, congrats! you have conducted a computational text analysis.

## Now for real

Use the European Parliament Proceedings from 1996 to 2011 available in text2map.corpora:

```{r}

data("corpus_europarl_subset", package = "text2map.corpora")

# Change name and add an ID per document
df_europarl <- corpus_europarl_subset |>
               rowid_to_column(var = "doc_id")

# Clean up the text
df_europarl <- df_europarl |>
  mutate(  # Text is the column name with the text (daah!)
    text = tolower(text),   # Into lower cases
    text = gsub("[[:punct:]]", " ", text),  # Remove punctuation
    text = gsub("[[:digit:]]+", " ", text), # Remove numbers
    text = gsub("[[:space:]]+", " ", text)  # Remove excess space
  )

# Tokenization
docs_tokens <- tokenize_words(df_europarl$text)

# Count the times in which each token occurs in each document
docs_freq <- lapply(docs_tokens, table) # table identify the matches and lapply does it for every document

# Create dataframe from the previous output
df_triplet <- melt(docs_freq)               # Three columns 
colnames(df_triplet) <- c("term", "freq", "doc_id")

# Look at the bottom
tail(df_triplet)

```

## Create a Document-Term-Matrix

Once we have tokenized and cleaned our text data, we can convert it into a structured format for analysis. The most common format is the **Document-Term Matrix (DTM)**, where rows represent documents and columns represent unique terms (usually words). Each cell in the matrix contains the frequency or presence of a term in a document. The DTM allows us to apply statistical techniques to text data by treating it as numerical input.

Let's keep using the European Parliament Proceedings:

```{r}
# Extract a list of all the (unique) tokens
vocab_unique <-sort(unique(unlist(docs_tokens)))

# Empty DTM
dtm <- matrix(
       data = 0,      # All 0s
       ncol = length(vocab_unique),    # N of unique tokens (columns)
       nrow = length(docs_freq),       # N of documents (rows)
       dimnames = list(df_europarl$doc_id, vocab_unique) # Row and columns names
)

# Fill the empty cells when a word in a document's token list matches a word in the column
for (i in seq_along(docs_freq)) {     # Loop for each entry in docs_freq
  freqs <- docs_freq[[i]]             # Gets words count
  words <- names(freqs)               # Extract the words/tokens for columns names
  dtm[i, words] <- as.integer(freqs)  # Assign the word counts
}

# Dimensions of the matrix

dim(dtm)                              # Rows and columns (takes time)

# We can not look at the whole DTM (documents x unique tokens!). Let's look at a selection of 4 columns
selection <- c("resumption", "session", "european", "you")
dtm[5001:5005, selection]             # Characters in selection for rows 5001 to 5005  
                                      # Clear workspace in Session menu if your laptop is slow

```

## Alternatives to create a Document-Term-Matrix

There are several packages available for text analysis in R, and because the Document-Term Matrix (DTM) is such a foundational structure, most of these packages provide built-in functions to create one. For example, `quanteda` uses the `dfm()` function, `text2map` provides `dtm_builder()`, and `tidytext` allows you to build DTMs using `cast_dfm()` or `cast_sparse()` after tokenizing and counting word frequencies. Regardless of the package, the resulting object is typically a sparse matrix, which can be converted into a standard matrix or data frame for further analysis. Choosing a package depends on your workflow preferences and the specific features you need for your analysis.

```{r}

# With quanteda
tokns <- quanteda::tokens(df_europarl$text)   # Reload the European Proceedings if closed
dtm_quanteda <- quanteda::dfm(tokns)

class(dtm_quanteda)
is(dtm_quanteda, "Matrix")

# With text2map
dtm_text2map <- df_europarl |> dtm_builder(text, doc_id)
class(dtm_text2map)
is(dtm_text2map, "Matrix")

# With tidytext
df_tidy <- df_europarl |>
  unnest_tokens(word, text) |>
  count(doc_id, word, sort = TRUE)

dtm_quanteda <- df_tidy |> cast_dfm(doc_id, word, n)
dtm_sparse <- df_tidy |> cast_sparse(doc_id, word, n)

#Convert the sparse matrices into standard data frames in R (Heavy)
dtm_base2<-as.matrix(dtm_quanteda)
dtm_base3<-as.matrix(dtm_sparse)
dtm_base4<-as.matrix(dtm_text2map)

```

## Challenge

Try the following excercise without looking at the solution below: *How many times does Beyonce use the words "love, baby, yeah, and know" in the first 10 songs?*

```{r}

# Change name and add an ID per document
df_beyonce <- corpus_beyonce |>
  rowid_to_column(var = "doc_id")

# Clean up the text
df_beyonce <- df_beyonce |>
  mutate(
    song_text = tolower(song_text),                     # Into lower cases
    song_text = gsub("[[:punct:]]", " ", song_text),    # Remove punctuation
    song_text = gsub("[[:digit:]]+", " ", song_text),   # Remove numbers
    song_text = gsub("[[:space:]]+", " ", song_text)    # Remove excess space
  )

# Tokenization
docs_tokens_beyonce <- tokenize_words(df_beyonce$song_text)

# Count the times in which each token occurs in each document
docs_freq_beyonce <- lapply(docs_tokens_beyonce, table)     

# Extract a list of all the unique tokens
vocab_unique_beyonce <-sort(unique(unlist(docs_tokens_beyonce)))

# Empty Document-Term-Matrix (DTM)
dtm_beyonce <- matrix(
  data = 0,                                # All 0s
  ncol = length(vocab_unique_beyonce),     # N of unique tokens (columns)
  nrow = length(docs_freq_beyonce),        # N of documents (rows)
  dimnames = list(df_beyonce$doc_id, vocab_unique_beyonce)  # Row and columns names
)

# Fill the empty cells when a word in a document's token list matches a word in the column
for (i in seq_along(docs_freq_beyonce)) {        # Loop for each entry in docs_freq
  freqs <- docs_freq_beyonce[[i]]                # Gets words count
  words <- names(freqs)         # Extract the words/tokens from the table's names
  dtm_beyonce[i, words] <- as.integer(freqs)     # Assign the word counts
}

# Dimensions of the matrix

dim(dtm_beyonce)                             # Rows and columns (takes time)

# We can not look at the whole DTM. Let's look at a selection
selection <- c("love", "baby", "yeah", "know")
dtm_beyonce[1:10, selection]    
```

# Weighting and Norming

After constructing a Document-Term Matrix (DTM), it's common to apply **weighting** or **norming** procedures to adjust the importance of different terms. Raw term counts can be misleading, as longer documents naturally contain more words. To correct for this, we often **normalize** the DTM by dividing each term count by the total number of terms in the corresponding document. This converts raw frequencies into relative frequencies, making documents more comparable regardless of their length.

## Relative term frequency

**Relative term frequency** refers to the proportion of times a term appears in a document relative to the total number of terms in that document. Instead of relying on raw word counts, which can be biased by document length, relative term frequency helps standardize the data by capturing how important a word is **within a specific document**.

Let's keep using the European Parliament Proceedings:

```{r}

# Relative term frequency

data("corpus_europarl_subset", package = "text2map.corpora")

df_europarl <- corpus_europarl_subset |>
   filter(language == "English") |>         # Onyly documents in English
   rowid_to_column(var = "doc_id") |>
   mutate(                                  # Text is the column name with the text
     text = tolower(text),                   # Into lower cases
     text = gsub("[[:punct:]]", " ", text),  # Remove punctuation
     text = gsub("[[:digit:]]+", " ", text), # Remove numbers
     text = gsub("[[:space:]]+", " ", text)  # Remove excess space
   )

# Creare a DTM (text2map package)
dtm <- df_europarl |> dtm_builder(text, doc_id)

# Total number of terms in each document (sum values in the rows)
doc_lenghts <- rowSums(dtm)

# Divide each cell (term count) by the total number of terms in its document 
dtm_rf <- dtm / doc_lenghts

# Inspect our DTM
dim(dtm_rf)                         # Number of rows and columns

dtm_rf_sub <- dtm_rf[seq_len(5), ]  # Subset first 5 rows to inspect

# Removing columns from the subsetted that have a total sum of 0
dtm_rf_sub <- dtm_rf_sub[, colSums(dtm_rf_sub) !=0]

# Show the 5 first columns with three decimals
round(dtm_rf_sub[, seq_len(5)], 3)

# Rows should sum 1 
rowSums(dtm_rf_sub)

# With quanteda
tokens <- tokens(df_europarl$text)
dtm_quanteda <- dfm(tokens)
quanteda_rf_dtm <- dfm_weight(dtm_quanteda, "prop")

```

## Term frequency / Inverse document frequency

Sometimes the more common words are not very informative. In that case, we would like to analyse the words occurring infrequently. Inverse document frequency is the number of documents divided by the total documents in which a term appears.

```{r}
df_beyonce <- corpus_beyonce |>
  rowid_to_column(var = "doc_id") |>
  mutate(                           
    song_text = tolower(song_text),                                             
    song_text = gsub("[[:punct:]]", " ",  song_text),  
    song_text = gsub("[[:digit:]]+", " ", song_text),
    song_text = gsub("[[:space:]]+", " ", song_text)  
  )


dtm <- df_beyonce |> dtm_builder(song_text, doc_id)

# Calculate relative term frequency
doc_lengths <- rowSums(dtm)
dtm_rf <- dtm / doc_lengths

# Rename into term frequency (non-relative)
tf <- dtm

# Inverse Document Frequency
idf <- log10(nrow(dtm) / colSums(dtm !=0)) # nrow gets the number of documents
                            # colsums counts how many documents contain each term
                            # log10 reduces the effect of extremely common or rare terms
                                           
# Element-wise multiplication of each column (term) in tf by its corresponding idf.
dtmtfidf <- tf * idf

# Element-wise multiplication of each column 
for (j in 1:ncol(dtm)) {
  dtmtfidf[, j] <- dtm[, j] * idf[j]
}

# Compute column sums for all metrics
df_terms <- data.frame(
  Term = colnames(dtm),
  tf = colSums(dtm),
  rf = colSums(dtm_rf),
  tfidf = colSums(dtmtfidf)
)

# Top 10 by each metric
top_tf <- df_terms %>% arrange(desc(tf)) %>% slice(1:10)
top_rf <- df_terms %>% arrange(desc(rf)) %>% slice(1:10)
top_tfidf <- df_terms %>% arrange(desc(tfidf)) %>% slice(1:10)

# Plotting function with color parameter

plot_top <- function(df, metric, title, fill_color) {
  ggplot(df, aes(x = reorder(Term, !!sym(metric)), y = !!sym(metric))) +
    geom_col(fill = fill_color) +
    coord_flip() +
    labs(x = "Term", y = metric, title = title) +
    theme_minimal(base_size = 13)
}

# Create each plot
plot_top_tf <- plot_top(top_tf, "tf", " ", "#1f77b4")
plot_top_rf <- plot_top(top_rf, "rf", " ", "#2ca02c")
plot_top_tfidf <- plot_top(top_tfidf, "tfidf", " ", "#d62728")

# Combine with patchwork
combined_plot <- plot_top_tf + plot_top_rf + plot_top_tfidf + plot_layout(ncol = 3)

# Show the combined plot
combined_plot

```
